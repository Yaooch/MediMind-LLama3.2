import json
import os
from datasets import load_dataset
from tqdm import tqdm

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# 1. ä¿å­˜è·¯å¾„ (ä¼šè‡ªåŠ¨åˆ›å»º)
OUTPUT_DIR = "./dataset/SFT_huotuo"
TRAIN_FILE_NAME = "train/huatuo_train.jsonl"
VAL_FILE_NAME = "val/huatuo_val.jsonl"

# 2. éªŒè¯é›†æ¯”ä¾‹
# è®¾ç½®ä¸º 0.01 è¡¨ç¤ºåˆ‡åˆ† 1% åšéªŒè¯é›†ï¼Œè®¾ç½®ä¸ºæ•´æ•°(å¦‚ 5000)è¡¨ç¤ºå›ºå®šåˆ‡å‡º 5000 æ¡
VAL_SIZE = 0.01

# 3. éšæœºç§å­ (ä¿è¯æ¯æ¬¡åˆ‡åˆ†çš„æ•°æ®æ˜¯ä¸€æ ·çš„)
SEED = 42
# ===============================================

def ensure_dir(directory):
    """å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º"""
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"ğŸ“‚ å·²åˆ›å»ºç›®å½•: {directory}")
    else:
        print(f"ğŸ“‚ ç›®å½•å·²å­˜åœ¨: {directory}")

def convert_to_sharegpt(hf_dataset, output_path):
    """å°† HuggingFace æ•°æ®é›†è½¬æ¢ä¸º ShareGPT æ ¼å¼å¹¶ä¿å­˜"""
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†å¹¶å†™å…¥: {output_path} ...")
    
    count = 0
    skipped = 0
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for item in tqdm(hf_dataset, desc="è½¬æ¢è¿›åº¦", unit="æ¡"):
            
            # 1. æå–å­—æ®µ (å…¼å®¹ä¸åŒå‘½åä¹ æƒ¯)
            q = item.get('question', item.get('input', '')).strip()
            a = item.get('answer', item.get('output', '')).strip()
            
            # 2. ç®€å•æ¸…æ´—
            if not q or not a:
                skipped += 1
                continue
                
            # 3. æ„å»º ShareGPT æ ¼å¼
            sharegpt_entry = {
                "conversations": [
                    {
                        "from": "human",
                        "value": q
                    },
                    {
                        "from": "gpt",
                        "value": a
                    }
                ]
            }
            
            # 4. å†™å…¥
            f.write(json.dumps(sharegpt_entry, ensure_ascii=False) + '\n')
            count += 1
            
    print(f"   âœ… å®Œæˆï¼æœ‰æ•ˆæ•°æ®: {count}, è·³è¿‡ç©ºæ•°æ®: {skipped}")
    return count

def main():
    # 1. å‡†å¤‡ç›®å½•
    ensure_dir(OUTPUT_DIR)
    
    # 2. åŠ è½½æ•°æ®é›†
    print("ğŸš€ æ­£åœ¨åŠ è½½ FreedomIntelligence/Huatuo26M-Lite æ•°æ®é›†...")
    try:
        # åŠ è½½å…¨é‡æ•°æ®
        ds = load_dataset("FreedomIntelligence/Huatuo26M-Lite", split="train")
        print(f"ğŸ“¦ åŸå§‹æ•°æ®æ€»é‡: {len(ds)} æ¡")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    # 3. åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (Train/Test Split)
    print(f"âœ‚ï¸ æ­£åœ¨åˆ‡åˆ†æ•°æ®é›† (éªŒè¯é›†å¤§å°: {VAL_SIZE}, éšæœºç§å­: {SEED})...")
    
    # ä½¿ç”¨ HuggingFace è‡ªå¸¦çš„åˆ‡åˆ†åŠŸèƒ½ï¼Œéå¸¸é«˜æ•ˆ
    split_ds = ds.train_test_split(test_size=VAL_SIZE, seed=SEED)
    
    train_ds = split_ds['train']
    val_ds = split_ds['test']
    
    print(f"   ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_ds)}")
    print(f"   ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_ds)}")

    # 4. åˆ†åˆ«è½¬æ¢å¹¶ä¿å­˜
    train_output_path = os.path.join(OUTPUT_DIR, TRAIN_FILE_NAME)
    val_output_path = os.path.join(OUTPUT_DIR, VAL_FILE_NAME)
    
    convert_to_sharegpt(train_ds, train_output_path)
    convert_to_sharegpt(val_ds, val_output_path)

    print("\n" + "="*40)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
    print(f"1ï¸âƒ£ è®­ç»ƒé›†: {train_output_path}")
    print(f"2ï¸âƒ£ éªŒè¯é›†: {val_output_path}")
    print("="*40)

if __name__ == "__main__":
    main()