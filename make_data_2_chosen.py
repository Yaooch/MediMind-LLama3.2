import json
import os
import time
from openai import OpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
# ================= é…ç½®åŒºåŸŸ =================
# 1. API é…ç½® (å»ºè®®ç”¨ GPT-4 æˆ– DeepSeek-V3 ç­‰å¼ºæ¨¡å‹)
API_BASE_URL = ""
API_KEY = ""       # ä½ çš„ API Key
MODEL_NAME = "MiniMaxAI/MiniMax-M2.1"        

# 2. æ–‡ä»¶è·¯å¾„
INPUT_FILE = "dpo_prompts_2k.jsonl"           # ä¸Šä¸€æ­¥ç”Ÿæˆçš„ Prompt æ–‡ä»¶
OUTPUT_FILE = "dpo_data_with_chosen.jsonl"    # ç»“æœä¿å­˜æ–‡ä»¶

# ğŸš€ å¹¶å‘è®¾ç½® (å…³é”®å‚æ•°)
MAX_WORKERS = 20  # åŒæ—¶è·‘å¤šå°‘ä¸ªè¯·æ±‚ (å»ºè®® 5-10ï¼Œå¤ªå¤šå®¹æ˜“è¢«å°/é™æµ)


SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸‰ç”²åŒ»é™¢çš„ä¸»æ²»åŒ»å¸ˆï¼Œæ­£åœ¨è¿›è¡Œåœ¨çº¿é—®è¯Šã€‚ä½ çš„å›ç­”éœ€è¦ä¸“ä¸šã€å®‰å…¨ã€æœ‰é€»è¾‘ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æµç•…çš„å¯¹è¯é£æ ¼ã€‚

# æ ¸å¿ƒè¦æ±‚
1. **å»æ ‡ç­¾åŒ–**ï¼šä¸è¦ä½¿ç”¨ã€æ ¸å¿ƒç»“è®ºã€‘ã€ã€è¯¦ç»†è§£æã€‘è¿™ç±»åƒµç¡¬çš„æ ‡é¢˜ã€‚è¯·é€šè¿‡è‡ªç„¶çš„åˆ†æ®µã€ç²—ä½“ (**Text**) å’Œåˆ—è¡¨æ¥ç»„ç»‡å†…å®¹ã€‚
2. **é»„é‡‘å›å¤ç»“æ„**ï¼š
   - **ç¬¬ä¸€æ®µ**ï¼šç›´æ¥ç»™å‡ºæ˜ç¡®çš„åŒ»å­¦ç»“è®ºæˆ–å»ºè®®ï¼ˆç›´å‡»ç—›ç‚¹ï¼‰ã€‚
   - **ä¸­é—´éƒ¨åˆ†**ï¼šè§£é‡ŠåŸå› ã€ç—…ç†æœºåˆ¶æˆ–çº æ­£æ‚£è€…çš„è®¤çŸ¥è¯¯åŒºï¼ˆç§‘æ™®æ•™è‚²ï¼‰ã€‚
   - **åç»­å»ºè®®**ï¼šä½¿ç”¨æ•°å­—åˆ—è¡¨ (1. 2. 3.) ç»™å‡ºå…·ä½“çš„æŠ¤ç†ã€ç”¨è¯æˆ–å°±åŒ»æŒ‡å¯¼ï¼ˆè¡ŒåŠ¨æŒ‡å—ï¼‰ã€‚
   - **ç»“å°¾**ï¼šå¿…è¦çš„ç¦å¿Œæé†’æˆ–å®‰æŠšï¼ˆå®‰å…¨å…œåº•ï¼‰ã€‚

# å®‰å…¨çº¢çº¿ (æœ€é«˜ä¼˜å…ˆçº§)
- å¦‚æœç”¨æˆ·æ¶‰åŠ**æ€¥æ•‘é”™è¯¯**ï¼ˆå¦‚æ˜è¿·å–‚æ°´ï¼‰ã€**é«˜å±è¡Œä¸º**ï¼ˆå¦‚è‡ªæ€ã€é…æ¯’è¯ï¼‰æˆ–**ä¸¥é‡è¯¯åŒº**ï¼Œå¿…é¡»åœ¨**ç¬¬ä¸€å¥è¯**ç”¨ä¸¥è‚ƒçš„è¯­æ°”è¿›è¡Œè­¦å‘Šå’Œçº æ­£ã€‚
- å¦‚æœæ¶‰åŠ**ä¸å­˜åœ¨çš„ç—…å**ï¼ˆå¹»è§‰ï¼‰ï¼Œè¯·è‡ªç„¶åœ°æŒ‡å‡ºï¼šâ€œåŒ»å­¦ä¸Šæ²¡æœ‰è¿™ä¸ªæ¦‚å¿µï¼Œæ‚¨æ˜¯å¦æŒ‡â€¦â€¦â€

# è¯­æ°”é£æ ¼
- ä¸“ä¸šä½†ä¸é«˜å†·ï¼Œä¸¥è°¨ä½†æœ‰æ¸©åº¦ã€‚
- åƒä¸€ä½è€å¿ƒçš„è€ä¸“å®¶åœ¨é¢å¯¹é¢å®å˜±æ‚£è€…ã€‚

# ç¤ºä¾‹ (ä»…ä¾›å‚è€ƒé£æ ¼ï¼Œä¸è¦ç…§æŠ„å†…å®¹)
User: æ„Ÿå†’å—“å­ç–¼èƒ½åƒå¤´å­¢å—ï¼Ÿ
Assistant: **ä¸å»ºè®®ç›´æ¥åƒå¤´å­¢ã€‚** å¤´å­¢æ˜¯æŠ—ç”Ÿç´ ï¼Œåªå¯¹ç»†èŒæœ‰æ•ˆï¼Œè€Œç»å¤§å¤šæ•°æ„Ÿå†’å’Œå—“å­ç–¼æ˜¯ç”±ç—…æ¯’å¼•èµ·çš„ï¼Œåƒå¤´å­¢ä¸ä»…æ— æ•ˆï¼Œè¿˜å¯èƒ½å¯¼è‡´è€è¯æ€§ã€‚
å»ºè®®æ‚¨é‡‡å–ä»¥ä¸‹æªæ–½ï¼š
1. **å¤šå–æ¸©æ°´**ï¼šä¿æŒå’½å–‰æ¹¿æ¶¦ã€‚
2. **å¯¹ç—‡ç”¨è¯**ï¼šå¦‚æœç–¼ç—›å‰§çƒˆï¼Œå¯ä½¿ç”¨æ¶¦å–‰ç‰‡æˆ–å¸ƒæ´›èŠ¬ç¼“è§£ã€‚
3. **è§‚å¯Ÿç—‡çŠ¶**ï¼šå¦‚æœå‡ºç°é«˜çƒ§ä¸é€€æˆ–æ‰æ¡ƒä½“åŒ–è„“ï¼Œè¯·åŠæ—¶å°±åŒ»æŸ¥è¡€è±¡ã€‚
ç‰¹åˆ«æé†’ï¼š**æœç”¨å¤´å­¢æœŸé—´ä¸¥ç¦é¥®é…’**ã€‚
"""

# ================= ä»£ç é€»è¾‘ =================

client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)

def process_single_item(line):
    """å¤„ç†å•æ¡æ•°æ®çš„å‡½æ•°"""
    try:
        record = json.loads(line)
        prompt = record.get("promp t", "")
        if not prompt:
            return None

        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000,
            timeout=60 # è®¾ç½®è¶…æ—¶é˜²æ­¢å¡æ­»
        )
        
        chosen_response = response.choices[0].message.content.strip()
        
        return {
            "prompt": prompt,
            "chosen": chosen_response,
            "source": "synthetic_expert"
        }
    except Exception as e:
        # print(f"âš ï¸ Error: {e}") # æŠ¥é”™å¤ªå¤šä¼šåˆ·å±ï¼Œå…ˆæ³¨é‡Šæ‰
        return None

def main():
    # 1. è¯»å–è¾“å…¥
    if not os.path.exists(INPUT_FILE):
        print("âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        input_lines = f.readlines()

    # 2. æ£€æŸ¥å·²å¤„ç† (æ–­ç‚¹ç»­ä¼ )
    processed_prompts = set()
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if "prompt" in data:
                        processed_prompts.add(data["prompt"])
                except:
                    pass
    
    # è¿‡æ»¤æ‰å·²ç»è·‘è¿‡çš„
    todos = [line for line in input_lines if json.loads(line).get("prompt") not in processed_prompts]
    print(f"ğŸš€ æ€»æ•°æ®: {len(input_lines)}, å·²å¤„ç†: {len(processed_prompts)}, å‰©ä½™ä»»åŠ¡: {len(todos)}")

    if not todos:
        print("âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼")
        return

    # 3. å¤šçº¿ç¨‹å¤„ç†
    with open(OUTPUT_FILE, "a", encoding="utf-8") as f_out:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤ä»»åŠ¡
            future_to_line = {executor.submit(process_single_item, line): line for line in todos}
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            for future in tqdm(as_completed(future_to_line), total=len(todos), desc="ğŸš€ å¤šçº¿ç¨‹ç”Ÿæˆä¸­"):
                result = future.result()
                if result:
                    # å†™å…¥ç»“æœ (åŠ é”å†™å…¥æ˜¯æ›´å¥½çš„ä¹ æƒ¯ï¼Œä½†åœ¨ Python GIL ä¸‹ç®€å•çš„ append write é—®é¢˜ä¸å¤§ï¼Œæˆ–è€…ç›´æ¥å•çº¿ç¨‹å†™)
                    f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                    f_out.flush()
                else:
                    # å¦‚æœå¤±è´¥äº†ï¼Œå¯ä»¥é€‰æ‹©ä¸å¤„ç†ï¼Œä¸‹æ¬¡è·‘è„šæœ¬ä¼šè‡ªåŠ¨é‡è¯•
                    pass

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()