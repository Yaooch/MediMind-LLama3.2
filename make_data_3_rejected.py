import json
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
# 1. ä½ çš„ SFT æ¨¡å‹è·¯å¾„ (å¿…é¡»æ˜¯åˆå¹¶åçš„å®Œæ•´æ¨¡å‹)
MODEL_PATH = "./merged-llama3-3b-dpo-v2" 

# 2. æ–‡ä»¶è·¯å¾„
# INPUT_FILE = "dpo_data_with_chosen.jsonl"  # ä¸Šä¸€æ­¥ç”Ÿæˆçš„åŒ…å« prompt å’Œ chosen çš„æ–‡ä»¶
# OUTPUT_FILE = "./dataset/DPO/dpo_train_data_final.jsonl" # æœ€ç»ˆç”¨äº DPO è®­ç»ƒçš„æ–‡ä»¶
INPUT_FILE = "dpo_test_answers.jsonl"  # ä¸Šä¸€æ­¥ç”Ÿæˆçš„åŒ…å« prompt å’Œ chosen çš„æ–‡ä»¶
OUTPUT_FILE = "dpo_test_answers_2.jsonl" # æœ€ç»ˆç”¨äº DPO è®­ç»ƒçš„æ–‡ä»¶


# 3. ç”Ÿæˆå‚æ•°
BATCH_SIZE = 16           # æ˜¾å­˜å¤Ÿå¤§(24G)å¯ä»¥å¼€åˆ° 8 æˆ– 16ï¼ŒMac æˆ– æ˜¾å­˜å°å°± 4
MAX_NEW_TOKENS = 512     # è®©å®ƒå¤šè¯´ç‚¹ï¼Œè¶Šé•¿è¶Šå®¹æ˜“æš´éœ²é”™è¯¯
TEMPERATURE = 0.9        # ç¨å¾®é«˜ç‚¹ï¼Œè¯±å¯¼å®ƒäº§ç”Ÿå¹»è§‰ (Temperatureè¶Šé«˜ï¼Œè¶Šå®¹æ˜“èƒ¡è¯´å…«é“)

# ================= ä»£ç é€»è¾‘ =================

def load_model():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    # è‡ªåŠ¨å¤„ç† padding token (Llama3 éœ€è¦ç‰¹åˆ«æ³¨æ„)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" # ç”Ÿæˆä»»åŠ¡å¿…é¡» left padding

    # åŠ è½½æ¨¡å‹
    # å¦‚æœæ˜¯ Macï¼Œtorch_dtype=torch.float16, device_map="mps" (æˆ–è€… auto)
    # å¦‚æœæ˜¯ Nvidia æ˜¾å¡ï¼Œtorch_dtype=torch.bfloat16, device_map="auto"
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,
        device_map="auto"
    )
    model.eval() # å¼€å¯è¯„ä¼°æ¨¡å¼
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer

def format_prompt_llama3(prompt, tokenizer):
    """
    æ„é€  Llama-3 çš„å¯¹è¯æ¨¡æ¿ã€‚
    å¿…é¡»è¦å’Œ SFT è®­ç»ƒæ—¶çš„ä¸€æ¨¡ä¸€æ ·ï¼
    """
    messages = [
        {"role": "user", "content": prompt}
    ]
    # apply_chat_template ä¼šè‡ªåŠ¨æ·»åŠ  <|begin_of_text|>, <|start_header_id|> ç­‰
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def main():
    model, tokenizer = load_model()

    # 1. è¯»å–æ•°æ®
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data_list = [json.loads(line) for line in f if line.strip()]

    print(f"ğŸ“Š å¾…å¤„ç†æ•°æ®: {len(data_list)} æ¡")

    # 2. å‡†å¤‡è¾“å‡ºæ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f_out:
        pass # æ¸…ç©ºæ–‡ä»¶

    # 3. æ‰¹é‡ç”Ÿæˆ
    # æ¯æ¬¡å¤„ç† BATCH_SIZE æ¡æ•°æ®
    for i in tqdm(range(0, len(data_list), BATCH_SIZE), desc="ç”Ÿæˆ Rejected"):
        batch_data = data_list[i : i + BATCH_SIZE]
        
        # 3.1 æå– Prompt å¹¶åº”ç”¨æ¨¡æ¿
        raw_prompts = [item["prompt"] for item in batch_data]
        formatted_prompts = [format_prompt_llama3(p, tokenizer) for p in raw_prompts]

        # 3.2 Tokenize
        inputs = tokenizer(
            formatted_prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=1024
        ).to(model.device)

        # 3.3 Generate (æ¨ç†)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,        # å¼€å¯é‡‡æ ·ï¼Œè®©å®ƒäº§ç”Ÿå¤šæ ·æ€§(å®¹æ˜“å‡ºé”™)
                temperature=TEMPERATURE,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
            )

        # 3.4 Decode (è§£ç )
        # åªä¿ç•™æ–°ç”Ÿæˆçš„ token (å»æ‰è¾“å…¥çš„ prompt)
        input_len = inputs.input_ids.shape[1]
        generated_tokens = outputs[:, input_len:]
        decoded_responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        # 3.5 ä¿å­˜ç»“æœ
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f_out:
            for j, item in enumerate(batch_data):
                # ç»„åˆæœ€ç»ˆçš„ DPO æ•°æ®é¡¹
                dpo_item = {
                    "prompt": item["prompt"],    # é—®é¢˜
                    # "chosen": item["chosen"],    # ä¸“å®¶å›ç­” (GPT-4)
                    "answer_sft": item["answer_sft"],    # ä¸“å®¶å›ç­” (GPT-4)
                    "answer_dpo": decoded_responses[j].strip() # ä½ çš„æ¨¡å‹ç”Ÿæˆçš„çƒ‚å›ç­”
                }
                f_out.write(json.dumps(dpo_item, ensure_ascii=False) + "\n")

    print(f"\nğŸ‰ æ­å–œï¼DPO æ•°æ®é›†æ„å»ºå®Œæˆï¼š{OUTPUT_FILE}")
    print("ç°åœ¨ä½ å¯ä»¥å»è·‘ dpo_training.py äº†ï¼")

if __name__ == "__main__":
    main()