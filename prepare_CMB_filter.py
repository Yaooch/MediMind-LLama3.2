import json
import torch
import os
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹ä¸å‚æ•°
# å»ºè®®ä½¿ç”¨ large æ¨¡å‹æ•ˆæœæ›´å¥½ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿæ”¹å› base
MODEL_NAME = 'BAAI/bge-base-zh-v1.5' 
TARGET_SIZE = 20000  # ç­›é€‰ä¿ç•™ 2w æ¡
TOP_K_AVG = 3       # è¯„åˆ†ç­–ç•¥ï¼šå–æœ€ç›¸ä¼¼çš„ 3 ä¸ªåˆ†æ•°æ±‚å¹³å‡

# 2. æ•°æ®é›† URL
CMB_TRAIN_URL = "https://hf-mirror.com/datasets/FreedomIntelligence/CMB/resolve/main/CMB-Exam/CMB-train/CMB-train-merge.json"
CMB_VAL_URL = "https://hf-mirror.com/datasets/FreedomIntelligence/CMB/resolve/main/CMB-Exam/CMB-val/CMB-val-merge.json"

# 3. è¾“å‡ºè·¯å¾„ (ä¼šè‡ªåŠ¨åˆ›å»ºå­ç›®å½•)
OUTPUT_DIR = "./dataset/SFT_CMB_filter"
OUTPUT_TRAIN_FILE = os.path.join(OUTPUT_DIR, 'train', "cmb_train_20k.json")
OUTPUT_VAL_FILE = os.path.join(OUTPUT_DIR, 'val', "cmb_val.json")
# ===========================================

def format_option_text(question, options_dict):
    """
    å°†é¢˜ç›®å’Œé€‰é¡¹æ‹¼æˆ Human è¾“å…¥æ–‡æœ¬
    """
    text = f"{question}____\n"
    valid_keys = ['A', 'B', 'C', 'D', 'E', 'F']
    for key in valid_keys:
        val = options_dict.get(key)
        if val:
            text += f"{key}. {val}\n"
    text += "\nç­”æ¡ˆï¼š"
    return text.strip()

def convert_to_sharegpt(item, include_explanation=False):
    """
    è½¬æ¢ä¸º ShareGPT æ ¼å¼
    å‚æ•° include_explanation: 
       - False: ä»…è¾“å‡º "ç­”æ¡ˆï¼šA" (ç”¨äºè®­ç»ƒé›†ï¼Œå¼ºè°ƒæ ¼å¼çº¦æŸ)
       - True: è¾“å‡º "ç­”æ¡ˆï¼šA\n\nè§£æï¼š..." (ç”¨äºéªŒè¯é›†ï¼Œæä¾›æ¨ç†é€»è¾‘)
    """
    # 1. æ„é€ è¾“å…¥
    human_input = format_option_text(item['question'], item['option'])
    
    # 2. æ„é€ è¾“å‡º
    gpt_output = f"{item['answer']}"
    
    # 3. æ ¹æ®ç­–ç•¥è¿½åŠ è§£æ
    if include_explanation:
        expl = item.get('explanation')
        # è¿‡æ»¤æ‰ None æˆ–è¿‡çŸ­çš„æ— æ•ˆè§£æ
        if expl and len(str(expl).strip()) > 5:
            gpt_output += f"\n\nè§£æï¼š{expl}"
    
    return {
        "conversations": [
            {"from": "human", "value": human_input},
            {"from": "gpt", "value": gpt_output}
        ]
    }

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(OUTPUT_TRAIN_FILE), exist_ok=True)
    os.makedirs(os.path.dirname(OUTPUT_VAL_FILE), exist_ok=True)
    
    print(f"ğŸš€ è®¾å¤‡: {device} | ç›®æ ‡ç­›é€‰: {TARGET_SIZE} æ¡ | æ¨¡å‹: {MODEL_NAME}")

    # ---------------------------------------------------------
    # 1. åŠ è½½ C-Eval (Anchors)
    # ---------------------------------------------------------
    print("ğŸ“‚ 1. åŠ è½½ C-Eval éªŒè¯é›† (Anchors)...")
    ceval_queries = []
    subsets = ["basic_medicine", "clinical_medicine", "physician"]
    for sub in subsets:
        try:
            ds = load_dataset("ceval/ceval-exam", sub, split="val", trust_remote_code=True)
            for item in ds:
                text = f"{item['question']}\nA. {item['A']}\nB. {item['B']}\nC. {item['C']}\nD. {item['D']}"
                ceval_queries.append(text)
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: åŠ è½½ C-Eval {sub} å¤±è´¥")
    
    print(f"âœ… C-Eval Anchor æ•°é‡: {len(ceval_queries)}")

    # ---------------------------------------------------------
    # 2. åŠ è½½ CMB æ•°æ®
    # ---------------------------------------------------------
    print("ğŸ“‚ 2. åŠ è½½ CMB æ•°æ®é›†...")
    ds_cmb_train = load_dataset("json", data_files=CMB_TRAIN_URL, split="train")
    ds_cmb_val = load_dataset("json", data_files=CMB_VAL_URL, split="train")
    
    print(f"   CMB Train åŸå§‹: {len(ds_cmb_train)}")
    print(f"   CMB Val åŸå§‹: {len(ds_cmb_val)}")

    # ---------------------------------------------------------
    # 3. è¿‡æ»¤ä¸ç¼–ç  (æ ¸å¿ƒä¿®æ”¹)
    # ---------------------------------------------------------
    print("ğŸ§  3. åŠ è½½ Embedding æ¨¡å‹å¹¶å‡†å¤‡æ•°æ®...")
    model = SentenceTransformer(MODEL_NAME, device=device)

    # 3.1 ç¼–ç  C-Eval
    query_embeddings = model.encode(ceval_queries, convert_to_tensor=True, normalize_embeddings=True)

    # 3.2 å‡†å¤‡ CMB Train (åªä¿ç•™å•é¡¹é€‰æ‹©é¢˜)
    print("   æ­£åœ¨è¿‡æ»¤CMBçš„å•é¡¹é€‰æ‹©é¢˜å¹¶å‡†å¤‡ CMB Train æ–‡æœ¬...")
    train_sentences = []
    valid_indices = [] # ã€å…³é”®ã€‘ç”¨äºè®°å½•ä¿ç•™ä¸‹æ¥çš„æ•°æ®åœ¨åŸå§‹æ•°æ®é›†ä¸­çš„ä¸‹æ ‡

    for idx, item in enumerate(tqdm(ds_cmb_train, desc="è¿‡æ»¤å•é€‰é¢˜")):
        # 1. å¿…é¡»æ˜¯å•é€‰é¢˜
        if item.get('question_type') != 'å•é¡¹é€‰æ‹©é¢˜':
            continue
        # 2. ç­”æ¡ˆå¿…é¡»æ˜¯å•ä¸ªå­—æ¯ (æ’é™¤ç­”æ¡ˆæ˜¯ 'ABCD' æˆ–ç©ºçš„è„æ•°æ®)
        ans = str(item.get('answer', '')).strip()
        if len(ans) != 1 or ans not in ['A', 'B', 'C', 'D', 'E', 'F']:
            continue
            
        text = format_option_text(item['question'], item['option'])
        train_sentences.append(text)
        valid_indices.append(idx) # è®°å½•åŸå§‹ ID

    print(f"   è¿‡æ»¤åå‰©ä½™: {len(train_sentences)} æ¡ (åŸå§‹ {len(ds_cmb_train)} æ¡)")
    print(f"   æ­£åœ¨ç¼–ç  CMB Train...")
    
    # æ‰¹é‡ç¼–ç 
    corpus_embeddings = model.encode(train_sentences, batch_size=64, convert_to_tensor=True, show_progress_bar=True, normalize_embeddings=True)

    # ---------------------------------------------------------
    # 4. ç›¸ä¼¼åº¦è®¡ç®—ä¸ç­›é€‰
    # ---------------------------------------------------------
    print(f"ğŸ” 4. è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç­›é€‰ Top-{TARGET_SIZE}...")
    
    cos_scores = util.cos_sim(corpus_embeddings, query_embeddings)
    top_k_vals, _ = torch.topk(cos_scores, k=TOP_K_AVG, dim=1)
    final_scores = torch.mean(top_k_vals, dim=1)
    
    # é€‰å– Top-N (è¿™é‡Œçš„ indices æ˜¯ train_sentences åˆ—è¡¨çš„ä¸‹æ ‡ï¼Œä¸æ˜¯åŸå§‹æ•°æ®é›†çš„)
    _, top_indices_local = torch.topk(final_scores, k=min(TARGET_SIZE, len(train_sentences)))
    selected_indices_local = top_indices_local.cpu().numpy().tolist()

    # ---------------------------------------------------------
    # 6. è½¬æ¢æ ¼å¼å¹¶ä¿å­˜
    # ---------------------------------------------------------
    print("ğŸ’¾ 6. è½¬æ¢æ ¼å¼å¹¶ä¿å­˜...")

    # 6.1 å¤„ç†ç­›é€‰åçš„ Train (æ— è§£æ)
    filtered_train_data = []
    for local_idx in selected_indices_local:
        original_idx = valid_indices[local_idx] # ã€å…³é”®ã€‘é€šè¿‡æ˜ å°„æ‰¾å›åŸå§‹æ•°æ®
        item = ds_cmb_train[original_idx]
        
        # è®­ç»ƒé›†ï¼šä¸å¸¦è§£æ
        filtered_train_data.append(convert_to_sharegpt(item, include_explanation=False))
    
    with open(OUTPUT_TRAIN_FILE, 'w', encoding='utf-8') as f:
        json.dump(filtered_train_data, f, indent=4, ensure_ascii=False)
    print(f"âœ… è®­ç»ƒé›†å·²ä¿å­˜: {OUTPUT_TRAIN_FILE} (å…± {len(filtered_train_data)} æ¡)")

    # 6.2 å¤„ç†å…¨éƒ¨ Val (æœ‰è§£æï¼Œä¸ç­›é€‰)
    # éªŒè¯é›†é€šå¸¸ä¸åšä¸¥æ ¼è¿‡æ»¤ï¼Œä¿ç•™å¤šæ ·æ€§ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥åŠ ä¸Šå•é€‰é¢˜è¿‡æ»¤ï¼Œè¿™é‡Œé»˜è®¤ä¿ç•™å…¨éƒ¨
    val_data = []
    for item in ds_cmb_val:
        # éªŒè¯é›†ï¼šå¸¦è§£æ
        val_data.append(convert_to_sharegpt(item, include_explanation=True))
        
    with open(OUTPUT_VAL_FILE, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, indent=4, ensure_ascii=False)
    print(f"âœ… éªŒè¯é›†å·²ä¿å­˜: {OUTPUT_VAL_FILE} (å…± {len(val_data)} æ¡)")

if __name__ == "__main__":
    main()