import json
import os
import torch
import random
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ¨¡å‹ä¸æ•°æ®
MODEL_NAME = 'BAAI/bge-base-zh-v1.5'
TARGET_SIZE = 20000  # ç­›é€‰å‡ºçš„æ€»æ•°æ®é‡
TOP_K_AVG = 3       # Top-3 å‡å€¼ç­–ç•¥

# 2. è¾“å‡ºç›®å½•é…ç½®
BASE_DIR = "./dataset/SFT_huatuo_filter_test_q"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
VAL_DIR = os.path.join(BASE_DIR, "val")

# 3. åˆ‡åˆ†æ¯”ä¾‹ (ä¾‹å¦‚ 90% è®­ç»ƒ, 10% éªŒè¯)
SPLIT_RATIO = 0.99 
# ===========================================

def save_json(data, folder, filename="data.json"):
    """è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºç›®å½•å¹¶ä¿å­˜JSON"""
    os.makedirs(folder, exist_ok=True)
    path = os.path.join(folder, filename)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"   å·²ä¿å­˜ {len(data)} æ¡æ•°æ®åˆ° -> {path}")

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"ğŸš€ è®¾å¤‡: {device} | ç›®æ ‡: {TARGET_SIZE}æ¡ | ç­–ç•¥: Top-{TOP_K_AVG} Avg")

    # 1. åŠ è½½ C-Eval (Anchors)
    print("ğŸ“‚ 1. åŠ è½½ C-Eval éªŒè¯é›†...")
    subsets = ["basic_medicine", "clinical_medicine", "physician"]
    ceval_queries = []
    for sub in subsets:
        try:
            # ds = load_dataset("ceval/ceval-exam", sub, split="val", trust_remote_code=True)
            ds = load_dataset("ceval/ceval-exam", sub, split="test", trust_remote_code=True)
            for item in ds:
                text = f"{item['question']} A.{item['A']} B.{item['B']} C.{item['C']} D.{item['D']}"
                ceval_queries.append(text)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {sub}: {e}")
    print(f"âœ… C-Eval é”šç‚¹: {len(ceval_queries)} æ¡")

    # 2. åŠ è½½ Huatuo (Candidates)
    print("ğŸ“‚ 2. åŠ è½½ Huatuo æ•°æ®é›†...")
    try:
        ds_huatuo = load_dataset("FreedomIntelligence/Huatuo26M-Lite", split="train", trust_remote_code=True)
        huatuo_list = ds_huatuo
        # huatuo_list = ds_huatuo.select(range(50000)) # è°ƒè¯•ç”¨
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        return

    # 3. ç¼–ç 
    print("ğŸ§  3. å‘é‡ç¼–ç ä¸­...")
    model = SentenceTransformer(MODEL_NAME, device=device)
    
    query_embeddings = model.encode(ceval_queries, convert_to_tensor=True, normalize_embeddings=True)
    
    # corpus_sentences = [f"{item['question']} {item['answer']}" for item in tqdm(huatuo_list, desc="å‡†å¤‡æ–‡æœ¬")]
    corpus_sentences = [f"{item['question']}" for item in tqdm(huatuo_list, desc="å‡†å¤‡æ–‡æœ¬")]

    corpus_embeddings = model.encode(corpus_sentences, batch_size=64, convert_to_tensor=True, show_progress_bar=True, normalize_embeddings=True)

    # 4. è®¡ç®— Top-3 å‡å€¼å¹¶æ’åº
    print("ğŸ” 4. è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç­›é€‰...")
    cos_scores = util.cos_sim(corpus_embeddings, query_embeddings)
    top_k_vals, _ = torch.topk(cos_scores, k=TOP_K_AVG, dim=1)
    final_scores = torch.mean(top_k_vals, dim=1)
    
    # é€‰å– Top-N ç´¢å¼•
    _, top_indices = torch.topk(final_scores, k=min(TARGET_SIZE, len(huatuo_list)))
    selected_indices = top_indices.cpu().numpy().tolist()

    # 5. æ ¼å¼è½¬æ¢ (ShareGPT)
    print("ğŸ”„ 5. æ ¼å¼è½¬æ¢ (Huatuo -> ShareGPT)...")
    filtered_data = []
    for idx in selected_indices:
        item = huatuo_list[idx]
        filtered_data.append({
            "conversations": [
                {"from": "human", "value": item['question']},
                {"from": "gpt", "value": item['answer']}
            ]
        })

    # 6. æ‰“ä¹±å¹¶åˆ‡åˆ† (Train/Val Split)
    print(f"âœ‚ï¸  6. åˆ‡åˆ†æ•°æ®é›† (æ¯”ä¾‹ {SPLIT_RATIO})...")
    random.seed(42) # å›ºå®šç§å­ï¼Œä¿è¯å¤ç°
    random.shuffle(filtered_data) # ã€å…³é”®ã€‘æ‰“ä¹±é¡ºåºï¼Œé˜²æ­¢éªŒè¯é›†åå·®

    split_point = int(len(filtered_data) * SPLIT_RATIO)
    train_data = filtered_data[:split_point]
    val_data = filtered_data[split_point:]

    # 7. ä¿å­˜åˆ°æŒ‡å®šç›®å½•
    print("ğŸ’¾ 7. ä¿å­˜æ–‡ä»¶...")
    # ä¿å­˜è®­ç»ƒé›†
    save_json(train_data, TRAIN_DIR, "train.json")
    # ä¿å­˜éªŒè¯é›†
    save_json(val_data, VAL_DIR, "val.json")

    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"   è®­ç»ƒé›†è·¯å¾„: {os.path.abspath(TRAIN_DIR)}/train.json")
    print(f"   éªŒè¯é›†è·¯å¾„: {os.path.abspath(VAL_DIR)}/val.json")

if __name__ == "__main__":
    main()